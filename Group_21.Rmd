---
title: "World happiness"
author: 'Group 21'
date: "2021/6/24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, comment = NA)
```

```{r libraries}
library(ggplot2)
library(dplyr)
library(moderndive)
library(ISLR)
library(skimr)
library(plotly)
library(tidyr)
library(jtools)
library(tidyverse)
library(kableExtra)
library(gridExtra)
```

```{r data, echo = FALSE, eval = TRUE}
happiness <- read_csv("/Users/yangwanqing/Desktop/happiness1.csv")
```

# Introduction{#sec:Intro}
# Exploratory Data Analysis {#sec:EDA}
# Visualization of the data {#sec:VIS}
# Formal Data Analysis {#sec:FDA}
To begin to analysis the world happiness dataset, we need to check the correlation between the six explanatory variables to avoid the problem of multicollinearity.
```{r,echo=FALSE}
happiness[,4:9]%>%
  cor()%>%
kable(caption = '\\label{tab:summaries} Correlation coefficient table of explanatory variables.') %>% kable_styling(latex_options = "hold_position")
```
From our correlation table we can see that the correlation between our Logged GDP and Healthy life expectancy is 0.859, which is a strong positive linear relationship.And the Logged GDP and Social support also have the high degree of collinearity, the correlation between this two variables is 0.785.So we remove Healthy life expectancy and Logged GDP.Then, using the remaining 4 explanatory variables to perform stepwise regression, and observe whether the remaining variables need to be eliminated.
```{r,echo=FALSE}
model<-lm(score~Social+Freedom+Generosity+corruption,data=happiness)
step(model)
```
According to the results of stepwise regression, we choose the model with the smallest AIC as the final model.Then,we fit the following linear model to the data.
$$\widehat{\mbox{score}}_{\mbox{i}} = \widehat{\alpha} +
{\widehat\beta}*{\mbox{Social}}_{\mbox{i}} +\widehat{\gamma}*{\mbox{Freedom}}_{\mbox{i}}+\widehat{\delta}*{\mbox{corruption}}_{\mbox{i}}$$

where

• the $\widehat{\mbox{score}}_{\mbox{i}}$: the happiness score of the $i\mbox{th}$ country.

• the $\widehat{\alpha}$: the intercept of the regression line.

• the $\widehat{\beta}$: the coefficient for the first explanatory variable ${\mbox{Social}}$.

• the $\widehat{\gamma}$: the coefficient for the second explanatory variable ${\mbox{Freedom}}$.

• the $\widehat{\delta}$: the coefficient for the second explanatory variable ${\mbox{corruption}}$.

When this model is fitted to the data, the following estimates of ${\alpha}$  (intercept) and ${\beta}$,${\gamma}$ and ${\delta}$ are returned:

```{r,echo=FALSE,fig.width = 13, fig.align = "center",fig.cap = "\\label{fig:resids} Scatterplots of the residuals by Position (left) and a histogram of the residuals (right)."}
model<-lm(score~Social+Freedom+corruption,data=happiness)
get_regression_table(model)%>%
  kable(caption = '\\label{tab:reg} Estimates of the parameters from the fitted linear
regression model.') %>% kable_styling(latex_options = 'HOLD_position')
```
According to this table, the coefficient for social support tells us that, taking all other variables in the model into account and holding them constant, there is an associated increase, on average,every increase of 1 unit in the social support score increases the happiness index score by approximately 5.63 units.In the same way, when the freedom score of life choice increases by 1 unit, the happiness index score also increases by approximately 2.23 units.On the contrary, for every increase of 1 unit in the score for corruption, the total score of happiness index decreases by 1.23 units.

Before we can continue to use the fitted model, we must check the model's assumptions. It is best to consider these according to the residual plot in Figure 2.
```{r residplots,echo=FALSE, fig.width = 13, fig.align = "center",fig.cap = "\\label{fig:resids} Scatterplots of the residuals by Social,Freedom,corruption,fitted value and the histogram of residuals.", fig.pos = 'H', message = FALSE}
regression.points <- get_regression_points(model)
p1 <- ggplot(regression.points, aes(x = Social, y = residual)) +
      geom_jitter(width = 0.5) +
      labs(x = "Social", y = "Residual") +
      geom_hline(yintercept = 0, col = "blue")
p2 <- ggplot(regression.points, aes(x = Freedom, y = residual)) +
      geom_jitter(width = 0.5) +
      labs(x = "Freedom", y = "Residual") +
      geom_hline(yintercept = 0, col = "blue")
p3 <- ggplot(regression.points, aes(x = corruption, y = residual)) +
      geom_jitter(width = 0.5) +
      labs(x = "corruption", y = "Residual") +
      geom_hline(yintercept = 0, col = "blue")
p4<-ggplot(regression.points, aes(x = score_hat, y = residual)) +
  geom_point() +
  labs(x = "Fitted values", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue", size = 1) 
p5<-ggplot(regression.points, aes(x = residual)) +
      geom_histogram(color = "white") +
      labs(x = "Residual")
grid.arrange(p1,p2,p3,p4,p5,ncol = 2)
```
The assumptions of the residuals having mean zero and constant variability across all values of the explanatory variable appear to be valid in this case.According to the three different explanatory variables scatter plots, it can be concluded that the residuals are uniformly distributed above and below the zero line, so the mean is 0. The residuals are randomly distributed around the zero line, and the distribution of the residuals is constant across all fitted values, so there is no obvious pattern or change in the variant.And also the histogram supports the assumption of normal distribution error.

# Conclusions {#sec:Conc}


